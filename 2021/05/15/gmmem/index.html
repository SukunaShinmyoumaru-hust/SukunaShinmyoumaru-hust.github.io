<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="[latexpage] 一些概率的解释 在这个条件下,我们把图片上没有动物的角的概率作为先验概率,图片上有动物的角并且是犀牛称为类条件概率 类条件概率是就是已知一个条件下，结果发生的概率。条件概率实际上把一个完整的问题集合S通过特征进行了划分，划分成S1&#x2F;S2&#x2F;S3…。类条件概率中的类指的是把造成结果的所有原因一一进行列举，分别讨论。 先验概率:事情还没有发生，根据以往经验和分">
<meta property="og:type" content="article">
<meta property="og:title" content="混合高斯模型和EM算法">
<meta property="og:url" content="https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/index.html">
<meta property="og:site_name" content="Suwa shrine">
<meta property="og:description" content="[latexpage] 一些概率的解释 在这个条件下,我们把图片上没有动物的角的概率作为先验概率,图片上有动物的角并且是犀牛称为类条件概率 类条件概率是就是已知一个条件下，结果发生的概率。条件概率实际上把一个完整的问题集合S通过特征进行了划分，划分成S1&#x2F;S2&#x2F;S3…。类条件概率中的类指的是把造成结果的所有原因一一进行列举，分别讨论。 先验概率:事情还没有发生，根据以往经验和分">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1234526/201710/1234526-20171017135438021-1879043887.png">
<meta property="og:image" content="https://pic1.zhimg.com/v2-61eef525c22533e2542f7f62a3add598_r.jpg">
<meta property="og:image" content="https://sukunahust.com/wp-content/uploads/2021/05/image-3-649x1024.png">
<meta property="og:image" content="https://sukunahust.com/wp-content/uploads/2021/05/image-4-1024x795.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cphi%5Cleft(x%5Cmu_i,%5CSigma_i%5Cright)=%5Cfrac%7B1%7D%7B%5Cleft(2%5Cpi%5Cright)%5E%5Cfrac%7Bd%7D%7B2%7D%5Cleft%5CSigma_i%5Cright%5E%5Cfrac%7B1%7D%7B2%7D%7D%5Cexp%7B%5Cleft(-%5Cfrac%7B%5Cleft(x-%5Cmu_i%5Cright)%5ET%5CSigma_i%5E%7B-1%7D%5Cleft(x-%5Cmu_i%5Cright)%7D%7B2%7D%5Cright)%7D%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=K">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctheta">
<meta property="og:image" content="https://sukunahust.com/wp-content/uploads/2021/05/image-2.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctheta">
<meta property="og:image" content="https://sukunahust.com/wp-content/uploads/2021/05/image-1.png">
<meta property="og:image" content="https://sukunahust.com/wp-content/uploads/2021/05/image-5.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cgamma_%7Bjk%7D+=+%5Cfrac%7B%5Calpha_%7Bk%7D%5Cphi(x_%7Bj%7D%7C%5Ctheta_%7Bk%7D)%7D%7B%5Csum_%7Bk=1%7D%5E%7BK%7D%7B%5Calpha_%7Bk%7D%5Cphi(x_%7Bj%7D%7C%5Ctheta_%7Bk%7D)%7D%7D,+j+=+1,2,...,N;+k+=+1,2,...,K">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmu_%7Bk%7D+=+%5Cfrac%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B(%5Cgamma_%7Bjk%7D%7Dx_%7Bj%7D)%7D%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D%7D,+k=1,2,...,K">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5CSigma_%7Bk%7D+=+%5Cfrac%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D(x_%7Bj%7D-%5Cmu_%7Bk%7D)(x_%7Bj%7D-%5Cmu_%7Bk%7D)%5E%7BT%7D%7D%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D%7D,+k+=+1,2,...,K">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmu_%7Bk%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D+=+%5Cfrac%7B%5Csum_%7Bj=1%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D%7D%7BN%7D,+k=1,2,...,K">
<meta property="article:published_time" content="2021-05-15T05:56:14.000Z">
<meta property="article:modified_time" content="2024-02-27T05:01:50.097Z">
<meta property="article:author" content="Sukuna">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images2017.cnblogs.com/blog/1234526/201710/1234526-20171017135438021-1879043887.png">
    
    
      
        
          <link rel="shortcut icon" href="/project/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/project/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/project/images/favicon-192x192.png">
        
      
    
    <!-- title -->
    <title>混合高斯模型和EM算法</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/project/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.1.1"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="顶部" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/project/">首页</a></li><!--
     --><!--
       --><li><a href="/project/about/">关于</a></li><!--
     --><!--
       --><li><a href="/project/archives/">归档</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/SukunaShinmyoumaru-hust/Hust-opensource-Xuejie">HUST-学解</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇" href="/project/2021/12/18/oslab1/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇" href="/project/2021/04/25/%E5%8D%8E%E4%B8%AD%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6verilog%E5%AE%9E%E9%AA%8C%E8%A7%A3%E6%9E%90/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&text=混合高斯模型和EM算法"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&is_video=false&description=混合高斯模型和EM算法"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=混合高斯模型和EM算法&body=Check out this article: https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&name=混合高斯模型和EM算法&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&t=混合高斯模型和EM算法"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%A6%82%E7%8E%87%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">1.</span> <span class="toc-text">一些概率的解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA"><span class="toc-number">2.</span> <span class="toc-text">贝叶斯决策论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">3.</span> <span class="toc-text">极大似然估计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">4.</span> <span class="toc-text">朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%AE%9E%E8%B7%B5"><span class="toc-number">5.</span> <span class="toc-text">贝叶斯分类器的实践</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">6.</span> <span class="toc-text">混合高斯分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EM%E7%AE%97%E6%B3%95"><span class="toc-number">7.</span> <span class="toc-text">EM算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8EGMM%E7%9A%84EM%E7%AE%97%E6%B3%95"><span class="toc-number">8.</span> <span class="toc-text">对于GMM的EM算法</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        混合高斯模型和EM算法
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Sukuna</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-05-15T05:56:14.000Z" class="dt-published" itemprop="datePublished">2021-05-15</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/project/categories/%E8%AE%BA%E6%96%87/">论文</a>
    </div>


      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>[latexpage]</p>
<h4 id="一些概率的解释"><a href="#一些概率的解释" class="headerlink" title="一些概率的解释"></a>一些概率的解释</h4><p><img src="https://images2017.cnblogs.com/blog/1234526/201710/1234526-20171017135438021-1879043887.png"></p>
<p>在这个条件下,我们把图片上没有动物的角的概率作为先验概率,图片上有动物的角并且是犀牛称为类条件概率</p>
<p>类条件概率是就是已知一个条件下，结果发生的概率。条件概率实际上把一个完整的问题集合S通过特征进行了划分，划分成S1&#x2F;S2&#x2F;S3…。类条件概率中的类指的是把造成结果的所有原因一一进行列举，分别讨论。</p>
<p>先验概率:事情还没有发生，根据以往经验和分析得到的概率，在事情发生之前，得到的事情（结果）发生的概率。比如，一次抛硬币实验，我们认为正面朝上的概率是0.5，这就是一种先验概率，在抛硬币前，我们只有常识。</p>
<p>后验概率:事情已经发生了，结果的发生的原因有很多，判断结果的发生是由哪个原因引起的概率</p>
<h4 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h4><p>假设有N种判别标记,$y&#x3D;{\theta_1,\theta_2,\theta_3……\theta_N}$,$\lambda_{ij}$为将一个真实的标记$\theta_j$错误地分成了$\theta_i$地的损失,基于后验概率可以定义把x分到$c_i$所产生的期望损失</p>
<p>于是有:$R(\theta_ix)&#x3D;\sum_{j&#x3D;1}^N\lambda_{ij}P(\theta_jx)$</p>
<p>在这里我们一般当i&#x3D;j的时候,$\lambda_{ij}&#x3D;0$,i≠j的时候,$\lambda_{ij}&#x3D;1$</p>
<p>所以说可以写出总体风险的表达式:$$R(h)&#x3D;\mathcal{E}_x[R(h(x)x)]$$</p>
<p>对于每个样本x,如果我们能最小化条件风险,我们就可以让总体风险减小,这个时候判定的准则就是最优的,因为我们的风险比较低,用数学式子表达一下:$h^*(x)&#x3D;argminR(cx)$</p>
<p>计算后验概率是我们需要考虑的,这里有两个模型需可以考虑,一个是判别模型,就是直接构造概率分布:$P(x,\theta)$,一个是生成模型,下面进行叙述:</p>
<p>在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验概率。<br>随机变量X的概率分布为 $f(x\theta)$，先验概率分布为$ f(\theta)$(样本空间中各类样本所占的比例)，根据贝叶斯定理，后验概率分布为$f(\thetax)$:这里的逻辑内涵是:$\theta$是输出(分类的标准),$x$是输入,那么$P(x\theta)$是类条件概率,我们又称作似然.</p>
<h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>现在我们已经有训练集$D$,并且$P(x\theta)$可以用一组向量进行表示,训练集的样本是独立同分布的:</p>
<p>现在我我们要利用训练集来估计参数,假设参数我们用$\theta_c$表示:,这个时候我们定义似然:$\prod_{x\in D_c}P(x\theta_c)$,这个时候我们就可以找到使得似然值最大的$\theta_c$</p>
<p>注意:这个时候$\theta_c$是类先验概率里面的一个参数,不能完全等同于c,这里只是说明方便把它替换了一下而已(因为贝叶斯学派认为$\theta$也是有分布的)</p>
<h4 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h4><p>从上面的分析中我们知道,我们很难得到$P(xc)$,因为P(xc)是需要我们构建复杂的模型进行生成的,我们假设x是独立同分布的,那么有:$$P(cx)&#x3D;\frac{P(c)}{P(x)}\proc _{i&#x3D;1}^d P(x_ic)$$,朴素贝叶斯分类器就是基于训练集D来估计先验概率和类条件概率</p>
<p>首先是先验概率:$$P(c)&#x3D;\frac{D_c}{D}$$</p>
<p>对于离散属性:我们让其条件概率为c类的所有元素之和和c类上取值为$x_i$的集合<br>对于连续属性:默认其为Gauss分布</p>
<p><img src="https://pic1.zhimg.com/v2-61eef525c22533e2542f7f62a3add598_r.jpg" alt="preview"></p>
<p>根据独立同分布原理,我们可以把类条件概率刻画为$$P(xc)&#x3D;\proc P(x_ic)$$</p>
<h4 id="贝叶斯分类器的实践"><a href="#贝叶斯分类器的实践" class="headerlink" title="贝叶斯分类器的实践"></a>贝叶斯分类器的实践</h4><p>下面用周志华的西瓜分类器3.0来作为例子:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">编号,色泽,根蒂,敲声,纹理,脐部,触感,密度,含糖率,好瓜</span><br><span class="line">1,青绿,蜷缩,浊响,清晰,凹陷,硬滑,0.697,0.46,是</span><br><span class="line">2,乌黑,蜷缩,沉闷,清晰,凹陷,硬滑,0.774,0.376,是</span><br><span class="line">3,乌黑,蜷缩,浊响,清晰,凹陷,硬滑,0.634,0.264,是</span><br><span class="line">4,青绿,蜷缩,沉闷,清晰,凹陷,硬滑,0.608,0.318,是</span><br><span class="line">5,浅白,蜷缩,浊响,清晰,凹陷,硬滑,0.556,0.215,是</span><br><span class="line">6,青绿,稍蜷,浊响,清晰,稍凹,软粘,0.403,0.237,是</span><br><span class="line">7,乌黑,稍蜷,浊响,稍糊,稍凹,软粘,0.481,0.149,是</span><br><span class="line">8,乌黑,稍蜷,浊响,清晰,稍凹,硬滑,0.437,0.211,是</span><br><span class="line">9,乌黑,稍蜷,沉闷,稍糊,稍凹,硬滑,0.666,0.091,否</span><br><span class="line">10,青绿,硬挺,清脆,清晰,平坦,软粘,0.243,0.267,否</span><br><span class="line">11,浅白,硬挺,清脆,模糊,平坦,硬滑,0.245,0.057,否</span><br><span class="line">12,浅白,蜷缩,浊响,模糊,平坦,软粘,0.343,0.099,否</span><br><span class="line">13,青绿,稍蜷,浊响,稍糊,凹陷,硬滑,0.639,0.161,否</span><br><span class="line">14,浅白,稍蜷,沉闷,稍糊,凹陷,硬滑,0.657,0.198,否</span><br><span class="line">15,乌黑,稍蜷,浊响,清晰,稍凹,软粘,0.36,0.37,否</span><br><span class="line">16,浅白,蜷缩,浊响,模糊,平坦,硬滑,0.593,0.042,否</span><br><span class="line">17,青绿,蜷缩,沉闷,稍糊,稍凹,硬滑,0.719,0.103,否</span><br></pre></td></tr></table></figure>

<p><img src="https://sukunahust.com/wp-content/uploads/2021/05/image-3-649x1024.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(&#x27;3.0.csv&#x27;, delimiter=&quot;,&quot;)</span><br><span class="line">del dataset[&#x27;编号&#x27;]</span><br><span class="line">print(dataset)</span><br><span class="line">#X保存了所有除了是和否的元素</span><br><span class="line">X = dataset.values[:, :-1]</span><br><span class="line">m, n = np.shape(X)</span><br><span class="line">print(m,n)</span><br><span class="line">#做四舍五入计算</span><br><span class="line">for i in range(m):</span><br><span class="line">    X[i, n - 1] = round(X[i, n - 1], 3)</span><br><span class="line">    X[i, n - 2] = round(X[i, n - 2], 3)</span><br><span class="line">#y保存了结果</span><br><span class="line">y = dataset.values[:, -1]</span><br><span class="line">columnName = dataset.columns</span><br><span class="line">colIndex = &#123;&#125;</span><br><span class="line">#每一列的元素进行编号</span><br><span class="line">for i in range(len(columnName)):</span><br><span class="line">    colIndex[columnName[i]] = i</span><br><span class="line"></span><br><span class="line">Pmap = &#123;&#125;  # 函数P很耗时间，而且经常会求一样的东西，因此我加了个记忆化搜索，用map存一下，避免重复计算，这里保存的是离散值</span><br><span class="line">kindsOfAttribute = &#123;&#125;  # kindsOfAttribute[0] = 3,因为有3种不同的类型的&quot;色泽&quot;</span><br><span class="line">for i in range(n):</span><br><span class="line">    kindsOfAttribute[i] = len(set(X[:, i]))</span><br><span class="line">continuousPara = &#123;&#125;  # 记忆一些参数的连续数据,以避免重复计算</span><br><span class="line">#保存好的和不好的元素</span><br><span class="line">goodList = []</span><br><span class="line">badList = []</span><br><span class="line">for i in range(len(y)):</span><br><span class="line">    if y[i] == &#x27;是&#x27;:</span><br><span class="line">        goodList.append(i)</span><br><span class="line">    else:</span><br><span class="line">        badList.append(i)</span><br><span class="line"></span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def P(colID, attribute, C):  # P(colName=attributeC) P(色泽=青绿是)</span><br><span class="line">    #已经预测过了,就直接返回一个三元组就行了</span><br><span class="line">    if (colID, attribute, C) in Pmap:</span><br><span class="line">        return Pmap[(colID, attribute, C)]</span><br><span class="line">    curJudgeList = []</span><br><span class="line">    if C == &#x27;是&#x27;:</span><br><span class="line">        curJudgeList = goodList</span><br><span class="line">    else:</span><br><span class="line">        curJudgeList = badList</span><br><span class="line">    ans = 0</span><br><span class="line">    #colID&gt;=6代表的是连续型变量</span><br><span class="line">    if colID &gt;= 6:</span><br><span class="line">        mean = 1</span><br><span class="line">        std = 1</span><br><span class="line">        if (colID, C) in continuousPara:</span><br><span class="line">            curPara = continuousPara[(colID, C)]</span><br><span class="line">            mean = curPara[0]</span><br><span class="line">            std = curPara[1]</span><br><span class="line">        else:</span><br><span class="line">            #求平均值和方差</span><br><span class="line">            curData = X[curJudgeList, colID]</span><br><span class="line">            mean = curData.mean()</span><br><span class="line">            std = curData.std()</span><br><span class="line">            # print(mean,std)</span><br><span class="line">            #保存元素</span><br><span class="line">            continuousPara[(colID, C)] = (mean, std)</span><br><span class="line">        #返回要测试的密度</span><br><span class="line">        ans = 1 / (math.sqrt(math.pi * 2) * std) * math.exp((-(attribute - mean) ** 2) / (2 * std * std))</span><br><span class="line">    else:</span><br><span class="line">        for i in curJudgeList:</span><br><span class="line">            if X[i, colID] == attribute:</span><br><span class="line">                ans += 1</span><br><span class="line">        ans = (ans + 1) / (len(curJudgeList) + kindsOfAttribute[colID])</span><br><span class="line">    Pmap[(colID, attribute, C)] = ans</span><br><span class="line">    # print(ans)</span><br><span class="line">    return ans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predictOne(single):</span><br><span class="line">    #先验概率</span><br><span class="line">    ansYes = math.log2((len(goodList) + 1) / (len(y) + 2))</span><br><span class="line">    ansNo = math.log2((len(badList) + 1) / (len(y) + 2))</span><br><span class="line">    for i in range(len(single)):  # 书上是连乘，但在实践中要把“连乘”通过取对数的方式转化为“连加”以避免数值下溢</span><br><span class="line">        ansYes += math.log2(P(i, single[i], &#x27;是&#x27;))</span><br><span class="line">        ansNo += math.log2(P(i, single[i], &#x27;否&#x27;))</span><br><span class="line">    # print(ansYes,ansNo,math.pow(2,ansYes),math.pow(2,ansNo))</span><br><span class="line">    if ansYes &gt; ansNo:</span><br><span class="line">        return &#x27;是&#x27;</span><br><span class="line">    else:</span><br><span class="line">        return &#x27;否&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predictAll(iX):</span><br><span class="line">    predictY = []</span><br><span class="line">    for i in range(m):</span><br><span class="line">        predictY.append(predictOne(iX[i]))</span><br><span class="line">    return predictY</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">predictY = predictAll(X)</span><br><span class="line">print(y)</span><br><span class="line">print(np.array(predictAll(X)))</span><br><span class="line">#输出测试结果</span><br><span class="line">confusionMatrix = np.zeros((2, 2))</span><br><span class="line">for i in range(len(y)):</span><br><span class="line">    if predictY[i] == y[i]:</span><br><span class="line">        if y[i] == &#x27;否&#x27;:</span><br><span class="line">            confusionMatrix[0, 0] += 1</span><br><span class="line">        else:</span><br><span class="line">            confusionMatrix[1, 1] += 1</span><br><span class="line">    else:</span><br><span class="line">        if y[i] == &#x27;否&#x27;:</span><br><span class="line">            confusionMatrix[0, 1] += 1</span><br><span class="line">        else:</span><br><span class="line">            confusionMatrix[1, 0] += 1</span><br><span class="line">print(confusionMatrix)</span><br><span class="line">acc = (confusionMatrix[0][0]+confusionMatrix[1][1])/17</span><br><span class="line">acc = str(acc)</span><br><span class="line">print(&#x27;acc: &#x27;+acc)</span><br></pre></td></tr></table></figure>

<p>输出:</p>
<p><img src="https://sukunahust.com/wp-content/uploads/2021/05/image-4-1024x795.png"></p>
<h4 id="混合高斯分布"><a href="#混合高斯分布" class="headerlink" title="混合高斯分布"></a>混合高斯分布</h4><p><strong>一维高斯分布函数</strong></p>
<p>$f(x)&#x3D;\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$</p>
<p><strong>（多元）高斯分布</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cphi%5Cleft(x%5Cmu_i,%5CSigma_i%5Cright)=%5Cfrac%7B1%7D%7B%5Cleft(2%5Cpi%5Cright)%5E%5Cfrac%7Bd%7D%7B2%7D%5Cleft%5CSigma_i%5Cright%5E%5Cfrac%7B1%7D%7B2%7D%7D%5Cexp%7B%5Cleft(-%5Cfrac%7B%5Cleft(x-%5Cmu_i%5Cright)%5ET%5CSigma_i%5E%7B-1%7D%5Cleft(x-%5Cmu_i%5Cright)%7D%7B2%7D%5Cright)%7D%5C" alt="[公式]"></p>
<p><strong>混合高斯分布</strong></p>
<p>GMM是一个生成模型，它假设数据是从多个高斯分布中生成的，可以这样理解生成流程：有 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]">个高斯分布，赋予每一个分布一个权重，每当生成一个数据时，就按权重的比例随机选择一个分布，然后按照该分布生成数据,就是隐变量，所以对于样本在给定参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 的条件下<strong>边际概率</strong></p>
<p>首先，明确变量与参数</p>
<p><img src="https://sukunahust.com/wp-content/uploads/2021/05/image-2.png"></p>
<p>其中参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 包含隐变量Z的概率分布，各个高斯的均值与协方差矩阵：</p>
<p><img src="https://sukunahust.com/wp-content/uploads/2021/05/image-1.png"></p>
<p>就是输入,​就是隐变量的值</p>
<p>协方差矩阵:左对角线是变量自己的相关系数，这个数越大圈里面的点扩散得越厉害。矩阵其他的数值就是变量本身跟其他变量的相关系数，这个数越大这个方向的椭圆就越尖。(矩阵的行列式就是协方差)</p>
<p>且样本的条件概率分布服从于$$ p(x)&#x3D;\sum\limits_{i&#x3D;1}^{n}{\pi_i\mathcal{N}(x\mu_i,\sum)}$$</p>
<p>我们用概率的角度来思考这个问题,我们发现: $$p(xz_k&#x3D;1)&#x3D;\mathcal{N}(x,\mu_k,\sum)$$,$$f(x_n)&#x3D;p(x_n)&#x3D;\sum_kp(z_k)p(x_nz_k)&#x3D;\sum_k\pi_k\mathcal{N}(x_n\mu_k,\sum)$$</p>
<p>其实我们最终的目的已经知道了,对应一些输入,我们要分成给定的​类,求出每一个类的核心,隐变量的离散分布,求出协方差矩阵来</p>
<p>总的来说:</p>
<p>高斯混合模型的概率分布为：</p>
<p><img src="https://sukunahust.com/wp-content/uploads/2021/05/image-5.png"></p>
<p>对于这个模型而言，参数$\theta &#x3D; \mu ,\sigma , \alpha$  ，也就是每个子模型的期望、方差（或协方差）、在混合模型中发生的概率。现在我们要求每个字模型的这些参数来作为分类手段</p>
<h4 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h4><p>还是上面的吃西瓜,对于一个西瓜的数据集,我们很难观察出所有西瓜的数据集成分,所以说我们就假设一个没有观测到的变量,我们把这个变量称为隐变量,现在我们想求隐变量的分布,就要用到EM算法,下面简要介绍其做法</p>
<p>1、根据已有的模型变量,推断出最佳的隐变量的参数<br>2、再根据已有的隐变量的参数,最大化模型变量</p>
<p>下面列出EM算法的数学表达:,我们假设大theta$\Theta$是模型的表面参数,$Z$是模型的隐参数,那么我们有:</p>
<p>E步:根据以前参数$\Theta$推导出隐变量分布,并且计算出对树似然关于Z的期望$Z^{new}&#x3D;E(Z\Theta)$</p>
<p>M步:$\Theta^{new}&#x3D;argmaxLL(\ThetaZ^{new})$根据参数推导出新的变量的极大似然值</p>
<h4 id="对于GMM的EM算法"><a href="#对于GMM的EM算法" class="headerlink" title="对于GMM的EM算法"></a>对于GMM的EM算法</h4><p>数学推导暂缺</p>
<ul>
<li>E-step：依据当前参数，计算每个数据 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 来自子模型 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 的可能性</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cgamma_%7Bjk%7D+=+%5Cfrac%7B%5Calpha_%7Bk%7D%5Cphi(x_%7Bj%7D%7C%5Ctheta_%7Bk%7D)%7D%7B%5Csum_%7Bk=1%7D%5E%7BK%7D%7B%5Calpha_%7Bk%7D%5Cphi(x_%7Bj%7D%7C%5Ctheta_%7Bk%7D)%7D%7D,+j+=+1,2,...,N;+k+=+1,2,...,K" alt="[公式]"></p>
<ul>
<li>M-step：计算新一轮迭代的模型参数</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu_%7Bk%7D+=+%5Cfrac%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B(%5Cgamma_%7Bjk%7D%7Dx_%7Bj%7D)%7D%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D%7D,+k=1,2,...,K" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5CSigma_%7Bk%7D+=+%5Cfrac%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D(x_%7Bj%7D-%5Cmu_%7Bk%7D)(x_%7Bj%7D-%5Cmu_%7Bk%7D)%5E%7BT%7D%7D%7B%5Csum_%7Bj%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D%7D,+k+=+1,2,...,K" alt="[公式]"> （用这一轮更新后的 <img src="https://www.zhihu.com/equation?tex=%5Cmu_%7Bk%7D" alt="[公式]"> ）</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D+=+%5Cfrac%7B%5Csum_%7Bj=1%7D%5E%7BN%7D%7B%5Cgamma_%7Bjk%7D%7D%7D%7BN%7D,+k=1,2,...,K" alt="[公式]"></p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/project/">首页</a></li>
        
          <li><a href="/project/about/">关于</a></li>
        
          <li><a href="/project/archives/">归档</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/SukunaShinmyoumaru-hust/Hust-opensource-Xuejie">HUST-学解</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%A6%82%E7%8E%87%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">1.</span> <span class="toc-text">一些概率的解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA"><span class="toc-number">2.</span> <span class="toc-text">贝叶斯决策论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">3.</span> <span class="toc-text">极大似然估计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">4.</span> <span class="toc-text">朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%AE%9E%E8%B7%B5"><span class="toc-number">5.</span> <span class="toc-text">贝叶斯分类器的实践</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">6.</span> <span class="toc-text">混合高斯分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EM%E7%AE%97%E6%B3%95"><span class="toc-number">7.</span> <span class="toc-text">EM算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8EGMM%E7%9A%84EM%E7%AE%97%E6%B3%95"><span class="toc-number">8.</span> <span class="toc-text">对于GMM的EM算法</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&text=混合高斯模型和EM算法"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&is_video=false&description=混合高斯模型和EM算法"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=混合高斯模型和EM算法&body=Check out this article: https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&title=混合高斯模型和EM算法"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&name=混合高斯模型和EM算法&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://sukunashinmyoumaru-hust.github.io/project/2021/05/15/gmmem/&t=混合高斯模型和EM算法"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020-2024
    Sukuna
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/project/">首页</a></li><!--
     --><!--
       --><li><a href="/project/about/">关于</a></li><!--
     --><!--
       --><li><a href="/project/archives/">归档</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/SukunaShinmyoumaru-hust/Hust-opensource-Xuejie">HUST-学解</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/project/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
